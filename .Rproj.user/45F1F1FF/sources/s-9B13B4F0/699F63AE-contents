---
output:
  html_document: default
  pdf_document: default
---

#### Problem 3 

```{r}
library(e1071)
set.seed (1)
x=matrix (rnorm (20*2) , ncol =2)
y=c(rep (-1,10) , rep (1 ,10) )
x[y==1 ,] = x[y==1,] + 1
plot(x, col =(3-y))

```

```{r}
dat=data.frame(x=x, y=as.factor(y))
library (e1071)
svmfit =svm(y~., data=dat , kernel ="linear", cost =10, scale =FALSE )
plot(svmfit , dat)
```

```{r}
svmfit$index
```

```{r}
summary (svmfit )
```
```{r}
svmfit =svm(y~., data=dat , kernel ="linear", cost =0.1, scale =FALSE )
plot(svmfit , dat)
svmfit$index
```

```{r}
set.seed (1)
tune.out=tune(svm ,y~.,data=dat,kernel ="linear",ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100)))
summary(tune.out)
```

```{r}
bestmod = tune.out$best.model
summary(bestmod)
```
```{r}
plot(bestmod,dat)
```


```{r}
xtest=matrix (rnorm (20*2) , ncol =2)
ytest=sample (c(-1,1) , 20, rep=TRUE)
xtest[ytest ==1 ,]= xtest[ytest ==1,] + 1
testdat =data.frame (x = xtest , y=as.factor(ytest))
```


```{r}
#for some mysterious reason the predict just won't return value, only null
#but the predicted y can be seen in html output -->
(ypred = predict(bestmod,testdat))
(ypred2 = predict(svmfit,testdat))
```

```{r}
ypred <- c( -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1) 
table(predict =ypred , truth= testdat$y )
```

```{r}
svmfit10 =svm(y~., data=dat , kernel ="linear", cost = 10, scale =FALSE )
(ypred10 = predict(svmfit,testdat))
#table(predict =ypred , truth= testdat$y )
#table(predict =ypred2 , truth= testdat$y )
#table(predict =ypred10 , truth= testdat$y )
```

```{r}
x[y==1 ,]= x[y==1 ,]+0.5
plot(x, col =(y+5) /2, pch =19)
```

```{r}
dat=data.frame(x=x,y=as.factor (y))
svmfit =svm(y~., data=dat , kernel ="linear", cost =1e5)
summary (svmfit)
plot(svmfit , dat)
```

```{r}
svmfit =svm(y~., data=dat , kernel ="linear", cost =1)
summary (svmfit)
plot(svmfit , dat)
```

### Task 3/3 a

```{r}
probYplus1 <- function(x1,x2)
{
  px1 = 1/16*exp(-(x1^2+x2^2)/32)
  px2 = exp(-(x1^2+x2^2)/2)
  return(px1/(px1+px2))
}
probYminus1 <- function(x1,x2)
{
  px1 = 1/16*exp(-(x1^2+x2^2)/32)
  px2 = exp(-(x1^2+x2^2)/2)
  return(px2/(px1+px2))
}
```

```{r}
library(MASS)
corr = c(1, 0, 0, 16)
sigma <- matrix(corr,nrow=2, ncol=2)
sample <- mvrnorm(200,c(0,0), Sigma = sigma)
sampley <- runif(200, min = 0, max = 1)
sampley <- probYplus1(sample[,1],sample[,2])>=sampley
smp <- data.frame(x = sample, y = as.factor(sampley))
head(smp)
```
```{r}
#plot(x, col =(y+5) /2, pch =19)
plot(smp$x.1,smp$x.2,col = (sampley + 5)/2, pch = 19)
```

```{r}
set.seed (1)
tune.out=tune(svm ,y~.,data=smp,kernel ="linear",ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100)))
summary(tune.out)
bestmod = tune.out$best.model
summary(bestmod)
plot(bestmod,smp)

#dat=data.frame(x=x,y=as.factor (y))
#svmfit =svm(y~., data=dat , kernel ="linear", cost =1e5)
#summary (svmfit)
#plot(svmfit , dat)
```
Confirm that linear classifier has no hope here! Error is 0.345 with all cost levels. Try polynomial kernel:

```{r}
set.seed (1)
tune.out=tune(svm ,y~.,data=smp,kernel ="polynomial",degree=2, ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100, 150)))
summary(tune.out)
bestmod = tune.out$best.model
summary(bestmod)
plot(bestmod,smp)

```
Bit better, now best error 0.155 with cost 10! Next try RBF kernel "radial"

```{r}
set.seed (1)
tune.out=tune(svm ,y~.,data=smp,kernel ="radial", ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100, 150)))
summary(tune.out)
bestmod = tune.out$best.model
summary(bestmod)
plot(bestmod,smp)

```
Still improving, best error 0.14 with cost 100

Now, modify the data by adding two more variables as x^2 for both x's: 

Try first the linear kernel:

```{r}
library(plyr)
smp <- mutate(smp, x.3 = x.1^2, x.4 = x.2^2)
set.seed (1)
tune.out=tune(svm ,y~.,data=smp,kernel ="linear",ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100, 150)))
summary(tune.out)
bestmod = tune.out$best.model
summary(bestmod)
```
Now, really, the linear kernel is able to produce error 0.125 with cost level 100. This is because x squared becomes positive for all x AND the results is smaller than x with x < 1 and it bigger if x > 1. For x.1 with mean = 0 & var = 1, it is more likely that x is close to zero AND for x.2 with mean = 0 & var = 16 it is more likely to be more than 1. This is why the model is better able to separate the two sets now as can be seen from the model graph created without the linear x's.

Drop off the linear x's, leave only squared x's and try one more time:

```{r}
smpSq <- smp
smpSq[,1]= smpSq [,4]
smpSq[,2]= smpSq [,5]
smpSq <- smpSq[-4:-5]
```

```{r}
head(smpSq)
```
```{r}
set.seed (1)
tune.out=tune(svm ,y~.,data=smpSq,kernel ="linear",ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100, 150)))
summary(tune.out)
bestmodSq = tune.out$best.model
summary(bestmodSq)
plot(bestmodSq,smpSq)
```


#### Task 3 b

```{r}
library(ISLR)
data (OJ)
smp_siz = floor(0.8*nrow(OJ))
set.seed(23)   # set seed to ensure you always have same random numbers generated
train_ind = sample(seq_len(nrow(OJ)),size = smp_siz)  # Randomly identifies therows equal to sample size ( defined in previous instruction) from  all the rows of Smarket dataset and stores the row number in train_ind
train =OJ[train_ind,] #creates the training dataset with row numbers stored in train_ind
test=OJ[-train_ind,]
head(train)
```

```{r}
svmModel <- svm(Purchase~., data = train, kernel = "linear", cost = 0.01)
summary(svmModel)
```

Applied linear kernel in the model fit with cost = 0.01. 


```{r}
trainPurch <- fitted(svmModel, train)
sum(trainPurch == train$Purchase)/nrow(train)
```


My predict function still does not work--> cut/paste predicted results from html View, save in csv file and upload back here.

```{r}
testPurch <- predict(svmModel,test)
class(testPurch)
#sum(testPurch == test$Purchase)/nrow(train)
#predPurch <- read.csv("./predictTask3b1.csv")
#predPurch <- predPurch$Purchase
#sum(predPurch == test$Purchase)/nrow(test)
```

With this linear kernel model, cost = 0.01, training error is 0.175, test error is 0.15

```{r}
tune.out=tune(svm,Purchase~.,data=train,kernel ="linear",ranges =list(cost=c(0.01, 0.1, 1,5,10)))
summary(tune.out)
bestmodPurc = tune.out$best.model
summary(bestmodPurc)

```

The best performance (error = 0.178) is obtained on model with cost = 1.00 (or with training data 0.175)

```{r}
trainPurch <- fitted(bestmodPurc, train)
1- sum(trainPurch == train$Purchase)/nrow(train)
```

```{r}
#(testPurch <- predict(bestmodPurc,test))
#sum(testPurch == test$Purchase)/nrow(train)
predPurch2 <- read.csv("./predictTask3b2.csv")
predPurch2 <- predPurch2$Purchase
1-sum(predPurch2 == test$Purchase)/nrow(test)
```

Error rate for test data is 0.136 with cost = 1.00, kernel = "linear"


Next try with kernel = "radial":

```{r}
tune.out=tune(svm,Purchase~.,data=train,kernel ="radial",ranges =list(cost=c(0.01, 0.1, 1,5,10)))
summary(tune.out)
bestmodPurc = tune.out$best.model
summary(bestmodPurc)
```

Best performance 0.182 is achieved with cost = 5.00

```{r}
trainPurch <- fitted(bestmodPurc, train)
1- sum(trainPurch == train$Purchase)/nrow(train)
```

```{r}
#(testPurch <- predict(bestmodPurc,test))
#sum(testPurch == test$Purchase)/nrow(train)
predPurch3 <- read.csv("./predictTask3b3.csv")
predPurch3 <- predPurch3$Purchase
1-sum(predPurch3 == test$Purchase)/nrow(test)
```
With kernel = "radial" the test error is 0.154 with cost = 5.00.

Test one more time, now with kernel = "polynomial", degree 2

```{r}
tune.out=tune(svm,Purchase~.,data=train,kernel ="polynomial",degree = 2, ranges =list(cost=c(0.01, 0.1, 1,5,10)))
summary(tune.out)
bestmodPurc = tune.out$best.model
summary(bestmodPurc)
```
```{r}
trainPurch <- fitted(bestmodPurc, train)
1- sum(trainPurch == train$Purchase)/nrow(train)
```

Best result (error = 0.176) obtained with cost = 10.00

Run model on test data:

```{r}
#(testPurch <- predict(bestmodPurc,test))
#sum(testPurch == test$Purchase)/nrow(train)
predPurch4 <- read.csv("./predictTask3b4.csv")
predPurch4 <- predPurch4$Purchase
1-sum(predPurch4 == test$Purchase)/nrow(test)

```
With kernel "polynomial" degree 2, cost = 10.00, the test error is 0.154
